{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2VisualVec for Sentence Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note answers the following two questions:\n",
    "1. How to load a trained Word2VisualVec model?\n",
    "2. How to predict visual features from a new sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following script to download and extract a Word2VisalVec model trained on flickr30k.\n",
    "Notice that please refer to [here](https://github.com/danieljf24/w2vv#required-data) to download the dataset \n",
    "\n",
    "\n",
    "```shell\n",
    "ROOTPATH=$HOME/trained_w2vv_model\n",
    "mkdir -p $ROOTPATH && cd $ROOTPATH\n",
    "\n",
    "# download and extract the pre-trained model\n",
    "wget http://lixirong.net/data/w2vv-tmm2018/flickr30k_trained_model.tar.gz\n",
    "tar zxf flickr30k_trained_model.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from basic.common import readPkl\n",
    "from w2vv_pred import W2VV_MS_pred, pred_mutual_error_ms\n",
    "from util.text import encode_text\n",
    "from util.text2vec import get_text_encoder\n",
    "from util.util import readImgSents \n",
    "from simpleknn.bigfile import BigFile\n",
    "from util.losser import get_losser\n",
    "from util.evaluation import i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flickr = False\n",
    "\n",
    "model_name = \"flickr30k_trained_model\" if use_flickr else \"1000chars_description_trained_model\"\n",
    "trainCollection = \"flickr30kenctrain\" if use_flickr else 'data_w2vvtrain'\n",
    "testCollection='data_w2vvtest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a trained Word2Visual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "08/12/2019 09:40:17 INFO [w2vv_pred.pyc.W2VV_MS_pred] loaded a trained Word2VisualVec model successfully\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.environ['HOME'],'trained_w2vv_model/' + model_name)\n",
    "abs_model_path = os.path.join(model_path, 'model.json')\n",
    "weight_path = os.path.join(model_path, 'best_model.h5')\n",
    "predictor = W2VV_MS_pred(abs_model_path, weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision of prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/12/2019 09:40:17 INFO [util/text2vec.pyc.Index2Vec] initializing ...\n",
      "08/12/2019 09:40:17 INFO [util/text2vec.pyc.BoW2VecFilterStop] initializing ...\n",
      "08/12/2019 09:40:17 INFO [util/text2vec.pyc.BoW2VecFilterStop] 50105 words\n",
      "08/12/2019 09:40:17 INFO [util/text2vec.pyc.AveWord2VecFilterStop] initializing ...\n",
      "[BigFile] 1743364x500 instances loaded from /home/oleh/VisualSearch/word2vec/flickr/vec500flickr30m\n"
     ]
    }
   ],
   "source": [
    "# setup multi-scale sentence vectorization\n",
    "opt = readPkl(os.path.join(model_path, 'option.pkl'))\n",
    "# opt.n_caption = 2\n",
    "\n",
    "rootpath=os.path.join(os.environ['HOME'],'VisualSearch')\n",
    "rnn_style, bow_style, w2v_style = opt.text_style.strip().split('@')\n",
    "text_data_path = os.path.join(rootpath, trainCollection, \"TextData\", \"vocabulary\", \"bow\", opt.rnn_vocab)\n",
    "bow_data_path = os.path.join(rootpath, trainCollection, \"TextData\", \"vocabulary\", bow_style, opt.bow_vocab)\n",
    "w2v_data_path = os.path.join(rootpath, \"word2vec\", opt.corpus,  opt.word2vec)\n",
    "\n",
    "text2vec = get_text_encoder(rnn_style)(text_data_path)\n",
    "bow2vec = get_text_encoder(bow_style)(bow_data_path)\n",
    "w2v2vec = get_text_encoder(w2v_style)(w2v_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity function\n",
    "losser = get_losser(opt.simi_fun)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2vec\n",
    "img_feats_path = os.path.join(rootpath, testCollection, 'FeatureData', opt.img_feature)\n",
    "img_feats = BigFile(img_feats_path)\n",
    "\n",
    "test_sent_file = os.path.join(rootpath, testCollection, 'TextData','%s.caption.txt' % testCollection)\n",
    "img_list, sents_id, sents = readImgSents(test_sent_file)\n",
    "all_errors = pred_mutual_error_ms(img_list, sents, predictor, text2vec, bow2vec, w2v2vec, img_feats, losser, opt=opt)\n",
    "\n",
    "\n",
    "# compute performance\n",
    "(r1i, r5i, r10i, medri, meanri) = i2t(all_errors, n_caption=opt.n_caption)\n",
    "print \"Image to text: %.1f, %.1f, %.1f, %.1f, %.1f\" % (r1i, r5i, r10i, medri, meanri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to text(flickr run) : 45.6, 72.1, 81.5, 2.0, 13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to text(up to 1000 words, vocab from flickr): 1.2, 3.3, 6.4, 115.0, 122.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to text(entire article, flickr vocab): 0.4, 2.1, 3.7, 362.0, 405.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Specific Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, isdir, join, exists, abspath\n",
    "from keras.preprocessing import image\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _remove_punctuation(text):\n",
    "    return re.sub(ur\"\\p{P}+\", \"\", text)\n",
    "\n",
    "def _getJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.loads(json.load(json_file))\n",
    "\n",
    "def _getTextFeatures(text_path):\n",
    "    data = _getJSON(text_path)\n",
    "    text = _remove_punctuation(data['text'].replace(\"\\n\", \" \"))\n",
    "    text = text[:1000].rsplit(' ', 1)[0]\n",
    "    # onyshchak: only checking first 1000 characters, will need to extract summary propely\n",
    "    return {\n",
    "        'id': data['id'],\n",
    "        'text': text,\n",
    "        \"title\": data['title']\n",
    "    }\n",
    "\n",
    "def _getImagesMeta(path):\n",
    "    return _getJSON(path)['img_meta']\n",
    "\n",
    "def _getValidImagePaths(article_path):\n",
    "    img_path = join(article_path, 'img/')\n",
    "    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n",
    "\n",
    "def _dump(path, data):\n",
    "    with open(path, 'w', encoding='utf8') as outfile:\n",
    "        json.dump(data, outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "def GetArticleData(article_path):\n",
    "    article_data = _getTextFeatures(join(article_path, 'text.json'))\n",
    "    article_data[\"img\"] = _getImagesMeta(join(article_path, 'img/', 'meta.json'))\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "def ReadArticles(data_path, offset=0, limit=None):\n",
    "    print(\"Reading in progress...\")\n",
    "    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n",
    "    limit = limit if limit else len(article_paths) - offset\n",
    "    \n",
    "    articles = []\n",
    "    for i in range(offset, offset + limit):\n",
    "        path = article_paths[i]\n",
    "        if (i - offset + 1) % 251 == 0: print(i - offset, \"articles have been read\")\n",
    "        article_data = GetArticleData(path)\n",
    "        articles.append(article_data)\n",
    "        if len(articles) >= limit: break  # useless?\n",
    "        \n",
    "    print(limit, \"articles have been read\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in progress...\n",
      "(250, 'articles have been read')\n",
      "(501, 'articles have been read')\n",
      "(752, 'articles have been read')\n",
      "(1003, 'articles have been read')\n",
      "(1254, 'articles have been read')\n",
      "(1505, 'articles have been read')\n",
      "(1756, 'articles have been read')\n",
      "(2007, 'articles have been read')\n",
      "(2258, 'articles have been read')\n",
      "(2509, 'articles have been read')\n",
      "(2760, 'articles have been read')\n",
      "(3011, 'articles have been read')\n",
      "(3262, 'articles have been read')\n",
      "(3513, 'articles have been read')\n",
      "(3764, 'articles have been read')\n",
      "(4015, 'articles have been read')\n",
      "(4266, 'articles have been read')\n",
      "(4517, 'articles have been read')\n",
      "(4768, 'articles have been read')\n",
      "(5019, 'articles have been read')\n",
      "(5270, 'articles have been read')\n",
      "(5521, 'articles have been read')\n",
      "(5638, 'articles have been read')\n",
      "CPU times: user 48.9 s, sys: 5.98 s, total: 54.9 s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles = ReadArticles('../data/', offset=0, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {i[\"filename\"]: i for a in articles for i in a['img']}\n",
    "images = np.array([x for x in images.values() if \"features\" in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = np.array([x[\"features\"] for x in images], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Hussein Obama II  January 20 2009 born August 4 1961 is an American attorney and politician who served as the 44th president of the United States from 2009 to 2017 A member of the Democratic Party he was the first African American to be elected to the presidency He previously served as a US senator from Illinois from 2005 to 2008 and an Illinois state senator from 1997 to 2004  Obama was born in Honolulu Hawaii After graduating from Columbia University in 1983 he worked as a community organizer in Chicago In 1988 he enrolled in Harvard Law School where he was the first black president of the Harvard Law Review After graduating he became a civil rights attorney and an academic teaching constitutional law at the University of Chicago Law School from 1992 to 2004 He represented the 13th district for three terms in the Illinois Senate from 1997 until 2004 when he ran for the US Senate He received national attention in 2004 with his March primary win his wellreceived July Democratic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.832839, 3.157705, 8.74132 , ..., 8.02501 , 9.083664, 7.874597]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = [x for x in articles if x[\"title\"] == \"Barack Obama\"][0]\n",
    "text = page[\"text\"]#page[\"img\"][0][\"description\"]\n",
    "print(text)\n",
    "rnn_vec, bow_w2v_vec = encode_text(opt, text2vec, bow2vec, w2v2vec, text)\n",
    "predicted_features = predictor.predict_one(rnn_vec, bow_w2v_vec).reshape(1, -1)\n",
    "predicted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/File%3A58th_Presidential_Inaugural_Ceremony_170120-D-BP749-1327.jpg\n"
     ]
    }
   ],
   "source": [
    "print(page[\"img\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69561876, -0.59700312, -0.73438151, ..., -0.73371196,\n",
       "       -0.65384362, -0.86627162])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = np.array(losser.calculate(predicted_features, img_features)[0])\n",
    "# res = res + 1\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-checking that `similarity` and `img_features` have the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70493889 -0.65125447 -0.75555891]\n",
      "[-0.75000157 -0.72129319 -0.882415  ]\n"
     ]
    }
   ],
   "source": [
    "print(similarity[:3])\n",
    "print(similarity[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7049388876239739], [-0.6512544742938519], [-0.7555589063030781]]\n",
      "[[-0.7500015651930437], [-0.721293186013536], [-0.8824149962743585]]\n"
     ]
    }
   ],
   "source": [
    "print(losser.calculate(img_features[:3], predicted_features))\n",
    "print(losser.calculate(img_features[-3:], predicted_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-checking that `images` and `img_features` have the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features = lambda img: np.array(img['features']).astype(np.float32)\n",
    "all([(get_features(images[i]) == img_features[i]).all() for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 double check that we have the same order, because similarities are very big and results bad\n",
    "* 5 then if doesnt work, train on single image per article (the most relevant one)\n",
    "* 4 finish with text2text similarity (the last priority)\n",
    "* 2 identify article with high precision and check images (is it for real?)\n",
    "* 3 check that we have the same precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9058710065953819, -0.2193203322630154)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(similarity), max(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Barack Obama'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page[u\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real images on `Barack Obama` Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/File%3A58th_Presidential_Inaugural_Ceremony_170120-D-BP749-1327.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarackObamaportrait.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_Iraq_2006.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_addresses_joint_session_of_Congress_2009-02-24.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_and_Bill_Clinton_%28cropped1%29.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_and_Matteo_Renzi_October_2016%2C_1.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_playing_basketball_with_members_of_Congress_and_Cabinet_secretaries_2.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_talks_with_Benjamin_Netanyahu_%288637772147%29.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_visiting_victims_of_2012_Aurora_shooting.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_welcomes_Shimon_Peres_in_the_Oval_Office.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABlackhawksWhiteHouse2010.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ACelebrating_a_new_America_-lovewins_58242_%2818588276403%29.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ADIG13623-230.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ADavid_Cameron_and_Barack_Obama_at_the_G20_Summit_in_Toronto.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AFlickr_Obama_Springfield_01.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AG8_leaders_watching_football.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AHandshake_between_the_President_and_Cuban_President_Ra%C3%BAl_Castro.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AJob_Growth_by_U.S._President_-_v1.png\n",
      "https://en.wikipedia.org/wiki/File%3ALugar-Obama.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AMerkel_an_Obama_Presidential_Medal_of_Freedom.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObama-venice-la.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObama_Macri_October_2017.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObama_cabinet_meeting_2009-11.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObama_family_portrait_in_the_Green_Room.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObama_signs_health_care-20100323.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObamamiltondavis1.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AObamas_at_church_on_Inauguration_Day_2013.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AP06409PS-0571_%283594694537%29.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AP112912PS-0444_-_President_Barack_Obama_and_Mitt_Romney_in_the_Oval_Office_-_crop.jpg\n",
      "https://en.wikipedia.org/wiki/File%3APPACA_Premium_Chart.jpg\n",
      "https://en.wikipedia.org/wiki/File%3APercentage_of_Individuals_in_the_United_States_Without_Health_Insurance%2C_1963-2015.png\n",
      "https://en.wikipedia.org/wiki/File%3APresident_Barack_Obama%2C_2012_portrait_crop.jpg\n",
      "https://en.wikipedia.org/wiki/File%3APresident_Barack_Obama.jpg\n",
      "https://en.wikipedia.org/wiki/File%3APresident_George_W._Bush_and_Barack_Obama_meet_in_Oval_Office.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AU.S._Total_Deficits_vs._National_Debt_Increases_2001-2010.png\n",
      "https://en.wikipedia.org/wiki/File%3AUS_President_Barack_Obama_taking_his_Oath_of_Office_-_2009Jan20.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AVladimir_Putin_and_Barack_Obama_%282015-09-29%29_01.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AVladimir_Putin_and_Barack_Obama_%282015-09-29%29_04.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ABarack_Obama_Sr_Jr.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ANobel_Prize.png\n",
      "https://en.wikipedia.org/wiki/File%3AObama_and_Biden_await_updates_on_bin_Laden.jpg\n"
     ]
    }
   ],
   "source": [
    "for x in page[\"img\"]:\n",
    "    print(x['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-10 ranked images predicted by the model for `Barack Obama` page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.90587101 -0.90522574 -0.90418146 -0.90394407 -0.8990243  -0.89894226\n",
      " -0.89850592 -0.89818072 -0.89746584 -0.89725399]\n"
     ]
    }
   ],
   "source": [
    "print(similarity[similarity.argsort()[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/File%3ACM_Punk_2.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AClintonSenate.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AMcCain25April2007Portsmouth.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ANational_Prayer_Service_Obama_Inauguration.jpg\n",
      "https://en.wikipedia.org/wiki/File%3ARIAN_archive_837790_Valentina_Tereshkova_and_Neil_Armstrong.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AThe-Dream_performing.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AGough_and_Margaret_Whitlam_-_Holt%27s_memorial_service.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AG8_leaders_watching_football.jpg\n",
      "https://en.wikipedia.org/wiki/File%3AMichael_Buffer_Fight_For_Children_Washington_DC_Nov_2007.JPG\n",
      "https://en.wikipedia.org/wiki/File%3AUrsula_K_Le_Guin.JPG\n"
     ]
    }
   ],
   "source": [
    "for x in images[similarity.argsort()[:10]]:\n",
    "    print(x['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict visual features of a novel sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "[ 6.1648006  5.2095037  8.546985  ...  7.519165  10.404728   7.7131257]\n"
     ]
    }
   ],
   "source": [
    "sent='a dog is playing with a cat'\n",
    "rnn_vec, bow_w2v_vec = encode_text(opt,text2vec,bow2vec,w2v2vec,sent)\n",
    "predicted_text_feat = predictor.predict_one(rnn_vec,bow_w2v_vec)\n",
    "print len(predicted_text_feat)\n",
    "print predicted_text_feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
